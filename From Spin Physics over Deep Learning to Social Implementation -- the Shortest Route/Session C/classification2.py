# -*- coding: utf-8 -*-
"""Classification2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SsPMtG57tBQdTkv77IheLQG5V3FuEXuQ

# 2 dimension classification task (Sinsoidal boundary)

Let us consider a simple example of classification task. 
Here we consider random 2-dimensional inputs with 2 class.

Following procedure is described as code.
1.   First, we define random inputs samples.
2.   Second, we define a neural netwrork with pytorch to classify.
3.   After that, we train the defined neural network with generated samples.

## Prepartion 
To use machine learning we should import several modules in colab.
"""

import numpy as np               # for matrix calculation
import matplotlib.pylab as plt      # for data visualization
import torch  # PyTorch deep learning framework
from torch import nn # nn means nerural network module in pytorch

"""## Data generation
Let's start.
First, we define random inputs samples with true partioning boundary.
"""

# number of sample points
N = 50

# generate sample points in 2-dimension with uniform distribution
x = np.random.uniform(low=-1, high=1, size=(N, 2))
t = np.zeros(N)

# define the label t=1 as upper part of the sinsoidal curve boundary
t[x[:, 1] - np.sin(2*np.pi* x[:,0]) > 0] = 1

"""OK, let's see the result of generation."""

# plot sample with label t == 0 as blue samples, t==1 as red samples
plt.figure(figsize=(5,5))

plt.plot(x[t==0, 0], x[t==0, 1], 'bo')
plt.plot(x[t==1, 0], x[t==1, 1], 'ro')

"""## Define a simple neural network

Here we consider a simple neural network.
There exists no hidden layer, that is, this isnot deep but shallow network.

"""

# Now let's construct neural network structure

class NNClassifier(nn.Module):
    '''
    2 dimensional input classifier.
    '''
    def __init__(self):
        '''constructor part, that defins the nn components'''
        super().__init__() 
        # network description
        num_inputs = 2 # input dimension
        num_outputs = 1 # output dimension
        self.l1 = nn.Linear(num_inputs, num_outputs, bias=True) # 
        self.sigmoid = nn.Sigmoid() # Activation function
    
    def forward(self, x):
        '''forward calculation'''
        ysum = self.l1(x)  # weighted sum of x
        y = self.sigmoid(ysum) # activate with sigmoid function
        return y

"""We make an instance of the defined neural network and define the loss function as the optimization measure."""

# make instance of a defined model
model = NNClassifier()

# define a loss function (Binary Cross Entropy)
loss_criterion = nn.BCELoss()

# define a optimization parameters
learning_rate = 0.01
moment_rate = 0.9
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=moment_rate)

"""## Training the neural network with generated data

Let us train(optimize) the network by decreasing the loss function.
"""

# epoch means the iterations of dataset to train
num_epoch = 10000

# translate inputs (x, t) for the appropriate shape.
# x is (N, 2) matrix
# t is (N, 1) matrix
# whole compoents should be 32bit float number
inputs = torch.from_numpy(x).to(torch.float32)
targets = torch.from_numpy(t.reshape(-1, 1)).to(torch.float32)

# to recode loss values in epochs
history = []

for epoch in range(num_epoch):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_criterion(outputs, targets) # loss means goodness of the classifier.

    # The parameters will be modified to decrease the loss
    loss.backward()
    optimizer.step()

    # record the history
    history.append(loss.item())

    if (epoch) % 1000 == 0:
        print(f'Epoch[{epoch:02d}/{num_epoch:02d}]: loss = {loss.item():04f}')

# If you'd like to save the data, comments out the following line
# torch.save(model.state_dict(), 'model1.pickl')

"""Please see the loss value that converges around 0.5.
This means the network could not decrease the loss. 

The following shows the history of loss in training. 
"""

plt.plot(np.array(history))
#plt.semilogy(np.array(hist)) # if you'd like to see log-scale
plt.title('Loss Evolution')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.grid()

"""Let us consider drawing the boundary obtained from the NN."""

# using meshgrid, we can describe relevance 

#making test input as xtest as mesh in [-1, 1] x [-1, 1]
MeshNum = 128
tplus = np.linspace(-1, 1, MeshNum)
tminus = np.linspace(1, -1, MeshNum)
xx, yy = np.meshgrid(tplus, tminus)
xtest = np.hstack((xx.reshape(MeshNum*MeshNum, 1), 
                   yy.reshape(MeshNum*MeshNum, 1)))
xtest = torch.from_numpy(xtest).to(torch.float32)

# predict whole points class labels
estimate = model(xtest).detach().numpy()

# show the result
plt.imshow(estimate.reshape(128, 128), extent=[-1, 1, -1, 1], cmap='bwr', alpha=0.5)
plt.colorbar()


# plot samples
plt.plot(x[t==0, 0], x[t==0, 1], 'bo', alpha=0.4)
plt.plot(x[t==1, 0], x[t==1, 1], 'ro', alpha=0.4)


# others
plt.xlim(-1, 1)
plt.ylim(-1, 1)

"""# Improving classifier performance

Above section, we adopt a shallow network, which has no hidden layer. 
The shallow network has not enough ability to represent the complex boundary. The deep network can overcome this restrictions.
So, we redefine the NN as follows.
"""

# Redefine 4-layer networks

class DeepNNClassifier(nn.Module):
    '''
    2 dimensional input classifier.
    '''
    def __init__(self):
        '''constructor part, that defins the nn components'''
        super().__init__() 
        # network description
        num_inputs = 2 # input dimension
        num_hidden1 = 10 # hidden layer1
        num_hidden2 = 10 # hidden layer2
        num_outputs = 1 # output dimension
        self.l1 = nn.Linear(num_inputs, num_hidden1, bias=True)
        self.l2 = nn.Linear(num_hidden1, num_hidden2, bias=True)
        self.l3 = nn.Linear(num_hidden2, num_outputs, bias=True)
        self.sigmoid = nn.Sigmoid() # Activation function
        self.relu = nn.ReLU() # ReLU activation
  
    def forward(self, x):
        '''forward calculation'''
        z1 = self.relu(self.l1(x))
        z2 = self.relu(self.l2(z1))
        y = self.sigmoid(self.l3(z2)) # activate with sigmoid function
        return y

# make instance of a defined model
deep_model = DeepNNClassifier()

# define a loss function (Binary Cross Entropy)
loss_criterion = nn.BCELoss()

# define a optimization parameters
learning_rate = 0.01
moment_rate = 0.9
optimizer = torch.optim.SGD(deep_model.parameters(), lr=learning_rate, momentum=moment_rate)

# epoch means the iterations of dataset to train
num_epoch = 10000

# translate inputs (x, t) for the appropriate shape.
# x is (N, 2) matrix
# t is (N, 1) matrix
# whole compoents should be 32bit float number
inputs = torch.from_numpy(x).to(torch.float32)
targets = torch.from_numpy(t.reshape(-1, 1)).to(torch.float32)

# to recode loss values in epochs
history = []

for epoch in range(num_epoch):
    optimizer.zero_grad()
    outputs = deep_model(inputs)
    loss = loss_criterion(outputs, targets) # loss means goodness of the classifier.

    # The parameters will be modified to decrease the loss
    loss.backward()
    optimizer.step()

    # record the history
    history.append(loss.item())

    if (epoch) % 1000 == 0:
        print(f'Epoch[{epoch:02d}/{num_epoch:02d}]: loss = {loss.item():04f}')

# If you'd like to save the data, comments out the following line
# torch.save(model.state_dict(), 'model1.pickl')

plt.plot(np.array(history))
#plt.semilogy(np.array(hist)) # if you'd like to see log-scale
plt.title('Loss Evolution')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.grid()

# using meshgrid, we can describe relevance 

#making test input as xtest as mesh in [-1, 1] x [-1, 1]
MeshNum = 128
tplus = np.linspace(-1, 1, MeshNum)
tminus = np.linspace(1, -1, MeshNum)
xx, yy = np.meshgrid(tplus, tminus)
xtest = np.hstack((xx.reshape(MeshNum*MeshNum, 1), 
                   yy.reshape(MeshNum*MeshNum, 1)))
xtest = torch.from_numpy(xtest).to(torch.float32)

# predict whole points class labels
estimate = deep_model(xtest).detach().numpy()

# show the result
plt.imshow(estimate.reshape(128, 128), extent=[-1, 1, -1, 1], cmap='bwr', alpha=0.5)
plt.colorbar()


# plot samples
plt.plot(x[t==0, 0], x[t==0, 1], 'bo', alpha=0.4)
plt.plot(x[t==1, 0], x[t==1, 1], 'ro', alpha=0.4)


# others
plt.xlim(-1, 1)
plt.ylim(-1, 1)