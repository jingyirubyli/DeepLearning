# -*- coding: utf-8 -*-
"""Classification1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ITpmuD_bTk2pUS8Rugr9p-dsewFjxhCd

# 2 dimension classification task (linear boundary)

Let us consider a simple example of classification task. 
Here we consider random 2-dimensional inputs with 2 class.

Following procedure is described as code.
1.   First, we define random inputs samples.
2.   Second, we define a neural netwrork with pytorch to classify.
3.   After that, we train the defined neural network with generated samples.

## Prepartion 
To use machine learning we should import several modules in colab.
"""

import numpy as np               # for matrix calculation
import matplotlib.pylab as plt      # for data visualization
import torch # import pytorch
from torch import nn # nn means nerural network module in pytorch

"""## Data generation
Let's start.
First, we define random inputs samples with true partioning boundary.
"""

# number of sample points
N = 1000

# here we define true boundary y(x) = w1[0] * x[0] + w1[1] * x[1] + w0 = 0
# true_w1 is 2-dimensional vector of true w1 and true_w0 is a true w0
true_w1 = np.array([1, np.random.rand()])
true_w0 = 0.0

# generate sample points in 2-dimension with uniform distribution
x = np.random.uniform(low=-1, high=1, size=(N, 2))

# define label; if.true_w1[0] * x[0] + true_w1[1] * x[1] + true_w0 > 0, the label t=1
# else label t = 0
# operator '@' means inner product
t = np.zeros(N)
t[x @ true_w1 + true_w0 > 0] = 1

"""OK, let's see the result of generation."""

# plot sample with label t == 0 as blue samples, t==1 as red samples
plt.figure(figsize=(5,5))

plt.plot(x[t==0, 0], x[t==0, 1], 'bo')
plt.plot(x[t==1, 0], x[t==1, 1], 'ro')

"""## Define a neural network

Here we consider a simple neural network.
"""

# Now let's construct neural network structure

class NNClassifier(nn.Module):
    '''
    2 dimensional input classifier.
    '''
    def __init__(self):
        '''constructor part, that defins the nn components'''
        super().__init__() 
        # network description
        num_inputs = 2 # input dimension
        num_outputs = 1 # output dimension
        self.l1 = nn.Linear(num_inputs, num_outputs, bias=True) # 
        self.sigmoid = nn.Sigmoid() # Activation function
    
    def forward(self, x):
        '''forward calculation'''
        ysum = self.l1(x)  # weighted sum of x
        y = self.sigmoid(ysum) # activate with sigmoid function
        return y

"""We make an instance of the defined neural network and define the loss function as the optimization measure."""

# make instance of a defined model
model = NNClassifier()

# define a loss function (Binary Cross Entropy)
loss_criterion = nn.BCELoss()

# define a optimization parameters
learning_rate = 0.01
moment_rate = 0.9
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=moment_rate)

"""## Training the neural network with generated data

Let us train(optimize) the network by decreasing the loss function.
"""

# epoch means the iterations of dataset to train
num_epoch = 10000

# translate inputs (x, t) for the appropriate shape.
# x is (N, 2) matrix
# t is (N, 1) matrix
# whole compoents should be 32bit float number
inputs = torch.from_numpy(x).to(torch.float32)
targets = torch.from_numpy(t.reshape(-1, 1)).to(torch.float32)

# to recode loss values in epochs
history = []

for epoch in range(num_epoch):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = loss_criterion(outputs, targets) # loss means goodness of the classifier.

    # The parameters will be modified to decrease the loss
    loss.backward()
    optimizer.step()

    # record the history
    history.append(loss.item())

    if (epoch) % 1000 == 0:
        print(f'Epoch[{epoch:02d}/{num_epoch:02d}]: loss = {loss.item():04f}')

# If you'd like to save the data, comments out the following line
# torch.save(model.state_dict(), 'model1.pickl')

"""Let us observe the history of loss in training."""

plt.plot(np.array(history))
#plt.semilogy(np.array(hist)) # if you'd like to see log-scale
plt.title('Loss Evolution')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.grid()

"""Here we compare the results in the parameter space."""

# extract parameters from the network
w1, w0 = model.l1.weight, model.l1.bias

w0 = float(w0)
w1 = w1[0].detach().numpy()   # translate torch tensor to numpy format.
print("estimate: w1, w0 = (%.3f, %.3f)" % (w1[1]/w1[0], w0/w1[0]))
print("true:     w1, w0 = (%.3f, %.3f)" % (true_w1[1]/true_w1[0], true_w0/true_w1[0]))

"""Let us consider drawing the boundary obtained from the NN and comparing the true one."""

# using meshgrid, we can describe relevance 

#making test input as xtest as mesh in [-1, 1] x [-1, 1]
MeshNum = 128
tplus = np.linspace(-1, 1, MeshNum)
tminus = np.linspace(1, -1, MeshNum)
xx, yy = np.meshgrid(tplus, tminus)
xtest = np.hstack((xx.reshape(MeshNum*MeshNum, 1), 
                   yy.reshape(MeshNum*MeshNum, 1)))
xtest = torch.from_numpy(xtest).to(torch.float32)

# predict whole points class labels
estimate = model(xtest).detach().numpy()

# show the result
plt.imshow(estimate.reshape(128, 128), extent=[-1, 1, -1, 1], cmap='bwr', alpha=0.5)
plt.colorbar()


# draw true line
xxt = np.linspace(-1, 1, MeshNum)
yyt = -true_w1[0]/true_w1[1] * xxt - true_w0 / true_w1[1]
plt.plot(xxt, yyt, 'g-', label='true')


# plot samples
plt.plot(x[t==0, 0], x[t==0, 1], 'bo', alpha=0.4)
plt.plot(x[t==1, 0], x[t==1, 1], 'ro', alpha=0.4)


# others
plt.legend()
plt.xlim(-1, 1)
plt.ylim(-1, 1)